#!/usr/bin/env python3
"""
weight_gap_comparison_0vs20_fixed.py
ä¿®å¤ç‰ˆæœ¬ - æ­£ç¡®å¤„ç†vocab_sizeå’Œé”™è¯¯
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import pickle
import os
import glob
import networkx as nx
from tqdm import tqdm
import json
from datetime import datetime

# ç¡®ä¿æ¨¡å‹å®šä¹‰æ–‡ä»¶å­˜åœ¨
try:
    from model import GPTConfig, GPT
except ImportError:
    print("âŒ Error: Cannot import 'model.py'")
    exit()

# ==================== 1. é…ç½®ä¸è¾…åŠ©å‡½æ•° ====================

class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    def __init__(self, mix_ratio, d_model=92):
        self.mix_ratio = mix_ratio
        self.d_model = d_model
        # ä½¿ç”¨CPUé¿å…CUDAé”™è¯¯
        self.device = torch.device('cpu')
        
        # æ¨¡å‹å‚æ•° - å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„vocab_size
        self.n_layer = 1
        self.n_head = 1
        self.n_embd = d_model
        self.vocab_size = 92  # ä¿®å¤ï¼šä½¿ç”¨92è€Œä¸æ˜¯94
        
        # è®¾ç½®checkpointç›®å½•æ¨¡å¼
        if mix_ratio == 0:
            self.checkpoint_pattern = f"out/composition_d{d_model}_0mix_*/ckpt_*.pt"
        else:
            self.checkpoint_pattern = f"out/composition_d{d_model}_{mix_ratio}mix_*/ckpt_*.pt"
        
        # åŠ è½½çœŸå®çš„å›¾ç»“æ„
        self.load_real_graph_structure()
    
    def load_real_graph_structure(self):
        """åŠ è½½çœŸå®çš„å›¾ç»“æ„"""
        # å°è¯•åŠ è½½çœŸå®çš„å›¾æ–‡ä»¶
        graph_paths = [
            'data/simple_graph/composition_90/composition_graph.graphml',
            'data/simple_graph/composition_graph.graphml',
        ]
        
        graph_loaded = False
        for graph_path in graph_paths:
            if os.path.exists(graph_path):
                try:
                    G = nx.read_graphml(graph_path)
                    self.G = nx.relabel_nodes(G, {node: int(node) for node in G.nodes()})
                    graph_loaded = True
                    print(f"âœ“ Loaded graph from: {graph_path}")
                    break
                except Exception as e:
                    print(f"âš ï¸ Error loading graph from {graph_path}: {e}")
        
        if not graph_loaded:
            print("âš ï¸ Creating default graph structure")
            # åˆ›å»ºé»˜è®¤çš„ä¸‰å±‚å›¾ç»“æ„
            self.G = nx.DiGraph()
            # S1: nodes 0-29, S2: nodes 30-59, S3: nodes 60-89
            # S1->S2 edges
            for i in range(30):
                for j in range(30, 60):
                    if (i + j) % 3 == 0:  # ç¡®å®šæ€§çš„è¾¹
                        self.G.add_edge(i, j)
            # S2->S3 edges
            for i in range(30, 60):
                for j in range(60, 90):
                    if (i + j) % 4 == 0:  # ç¡®å®šæ€§çš„è¾¹
                        self.G.add_edge(i, j)
        
        # ç”Ÿæˆé‚»æ¥çŸ©é˜µ - æ³¨æ„å¤§å°æ˜¯92x92
        nodelist = sorted(self.G.nodes())
        A_true_90 = nx.to_numpy_array(self.G, nodelist=nodelist)
        
        # ä¿®å¤ï¼šåˆ›å»º92x92çš„çŸ©é˜µï¼Œä¸æ˜¯94x94
        self.A_true = np.zeros((self.vocab_size, self.vocab_size))
        # tokens 0,1æ˜¯ç‰¹æ®Štokenï¼Œå®é™…èŠ‚ç‚¹ä»2å¼€å§‹
        if A_true_90.shape[0] == 90:
            self.A_true[2:92, 2:92] = A_true_90
        else:
            # å¦‚æœå›¾ä¸æ˜¯90ä¸ªèŠ‚ç‚¹ï¼Œè°ƒæ•´å¤§å°
            min_size = min(A_true_90.shape[0], 90)
            self.A_true[2:2+min_size, 2:2+min_size] = A_true_90[:min_size, :min_size]
        
        print(f"âœ“ Adjacency matrix created (shape: {self.A_true.shape})")
        print(f"  S1->S2 edges: {np.sum(self.A_true[2:32, 32:62])}")
        print(f"  S2->S3 edges: {np.sum(self.A_true[32:62, 62:92])}")

def find_checkpoint(mix_ratio, d_model, iteration):
    """æŸ¥æ‰¾ç‰¹å®šè¿­ä»£çš„checkpoint"""
    if mix_ratio == 0:
        patterns = [
            f"out/composition_d{d_model}_0mix_*/ckpt_{iteration}.pt",
            f"out_d{d_model}/composition_mix0_*/ckpt_mix0_*{iteration}.pt",
        ]
    else:
        patterns = [
            f"out/composition_d{d_model}_{mix_ratio}mix_*/ckpt_{iteration}.pt",
            f"out_d{d_model}/composition_mix{mix_ratio}_*/ckpt_mix{mix_ratio}_*{iteration}.pt",
        ]
    
    for pattern in patterns:
        files = glob.glob(pattern)
        if files:
            return files[0]
    return None

def extract_W_M_prime(checkpoint_path, config):
    """æå–W'_MçŸ©é˜µï¼ˆåŒ…å«æ®‹å·®è¿æ¥ï¼‰"""
    try:
        # å°è¯•ä½¿ç”¨weights_only=FalseåŠ è½½
        checkpoint = torch.load(checkpoint_path, map_location=config.device, weights_only=False)
        
        # è·å–æ¨¡å‹é…ç½®
        model_args = checkpoint.get('model_args', {})
        if not model_args:
            model_args = {
                'n_layer': config.n_layer,
                'n_head': config.n_head,
                'n_embd': config.n_embd,
                'vocab_size': config.vocab_size,
                'block_size': 512,
                'dropout': 0.0,
                'bias': False
            }
        
        # ç¡®ä¿vocab_sizeæ­£ç¡®
        model_args['vocab_size'] = config.vocab_size
        
        gptconf = GPTConfig(**model_args)
        model = GPT(gptconf).to(config.device)
        
        # åŠ è½½çŠ¶æ€å­—å…¸ï¼Œå¿½ç•¥å¤§å°ä¸åŒ¹é…
        state_dict = checkpoint['model']
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        
        # æå–W'_MçŸ©é˜µ - åªå¤„ç†vocab_sizeä¸ªtoken
        W_M_prime = []
        with torch.no_grad():
            for i in range(config.vocab_size):
                # Token embedding
                token_emb = model.transformer.wte(torch.tensor([i], device=config.device))
                # FFN output
                ffn_out = model.transformer.h[0].mlp(token_emb)
                # æ®‹å·®è¿æ¥
                combined = token_emb + ffn_out
                # é€šè¿‡lm_head
                logits = model.lm_head(combined)
                W_M_prime.append(logits.squeeze().cpu().numpy()[:config.vocab_size])  # ç¡®ä¿è¾“å‡ºå¤§å°æ­£ç¡®
        
        return np.array(W_M_prime)
        
    except Exception as e:
        print(f"    Error extracting W_M_prime: {e}")
        raise

def calculate_path_statistics(W_M_prime, A_true, path_type):
    """è®¡ç®—ç‰¹å®šè·¯å¾„ç±»å‹çš„ç»Ÿè®¡ä¿¡æ¯"""
    if path_type == 'S1->S2':
        rows, cols = np.s_[2:32], np.s_[32:62]
    elif path_type == 'S2->S3':
        rows, cols = np.s_[32:62], np.s_[62:92]
    elif path_type == 'S1->S3':
        rows, cols = np.s_[2:32], np.s_[62:92]
    else:
        raise ValueError(f"Invalid path_type: {path_type}")
    
    # æå–å­çŸ©é˜µ
    W_sub = W_M_prime[rows, cols]
    A_sub = A_true[rows, cols]
    
    # åˆ›å»ºæ©ç 
    edge_mask = (A_sub == 1)
    non_edge_mask = (A_sub == 0)
    
    # è®¡ç®—ç»Ÿè®¡é‡
    stats = {
        'num_edges': np.sum(edge_mask),
        'num_non_edges': np.sum(non_edge_mask)
    }
    
    if stats['num_edges'] > 0:
        stats['avg_edge_weight'] = np.mean(W_sub[edge_mask])
        stats['std_edge_weight'] = np.std(W_sub[edge_mask])
    else:
        stats['avg_edge_weight'] = np.nan
        stats['std_edge_weight'] = np.nan
    
    if stats['num_non_edges'] > 0:
        stats['avg_non_edge_weight'] = np.mean(W_sub[non_edge_mask])
        stats['std_non_edge_weight'] = np.std(W_sub[non_edge_mask])
    else:
        stats['avg_non_edge_weight'] = 0
        stats['std_non_edge_weight'] = 0
    
    # è®¡ç®—gap
    if stats['num_edges'] > 0:
        stats['gap'] = stats['avg_edge_weight'] - stats['avg_non_edge_weight']
    else:
        stats['gap'] = np.nan
    
    return stats

# ==================== 2. æ•°æ®æ”¶é›†å‡½æ•° ====================

def collect_evolution_data(configs, iterations):
    """æ”¶é›†ä¸¤ä¸ªæ¨¡å‹çš„æ¼”åŒ–æ•°æ®"""
    evolution_data = {}
    
    for model_name, config in configs.items():
        print(f"\nğŸ“Š Processing {model_name} model...")
        
        model_data = {
            'S1->S2': {'edge': [], 'non_edge': [], 'gap': []},
            'S2->S3': {'edge': [], 'non_edge': [], 'gap': []},
            'S1->S3': {'edge': [], 'non_edge': [], 'gap': []}
        }
        
        found_checkpoints = 0
        for iteration in tqdm(iterations, desc=f"Loading {model_name} checkpoints"):
            checkpoint_path = find_checkpoint(config.mix_ratio, config.d_model, iteration)
            
            if checkpoint_path is None:
                # å¡«å……NaN
                for path_type in model_data.keys():
                    model_data[path_type]['edge'].append(np.nan)
                    model_data[path_type]['non_edge'].append(np.nan)
                    model_data[path_type]['gap'].append(np.nan)
                continue
            
            try:
                # æå–W'_MçŸ©é˜µ
                W_M_prime = extract_W_M_prime(checkpoint_path, config)
                found_checkpoints += 1
                
                # è®¡ç®—å„è·¯å¾„ç»Ÿè®¡
                for path_type in ['S1->S2', 'S2->S3', 'S1->S3']:
                    stats = calculate_path_statistics(W_M_prime, config.A_true, path_type)
                    model_data[path_type]['edge'].append(stats['avg_edge_weight'])
                    model_data[path_type]['non_edge'].append(stats['avg_non_edge_weight'])
                    model_data[path_type]['gap'].append(stats['gap'])
                    
            except Exception as e:
                print(f"  âš ï¸ Error at iteration {iteration}: {e}")
                for path_type in model_data.keys():
                    model_data[path_type]['edge'].append(np.nan)
                    model_data[path_type]['non_edge'].append(np.nan)
                    model_data[path_type]['gap'].append(np.nan)
        
        print(f"  âœ“ Found {found_checkpoints}/{len(iterations)} checkpoints")
        evolution_data[model_name] = model_data
    
    return evolution_data

# ==================== 3. å¯è§†åŒ–å‡½æ•°ï¼ˆä¿®å¤ç‰ˆï¼‰ ====================

def plot_comprehensive_comparison(evolution_data, iterations, save_dir):
    """ç”Ÿæˆç»¼åˆå¯¹æ¯”å›¾ï¼ˆ3x3å¸ƒå±€ï¼‰- ä¿®å¤ç‰ˆ"""
    if not any(evolution_data.values()):
        print("âš ï¸ No data to plot")
        return
    
    fig, axes = plt.subplots(3, 3, figsize=(18, 14))
    fig.suptitle('Weight Gap Evolution: 0% vs 20% Mix Models', fontsize=16, fontweight='bold')
    
    colors = {'0% mix': 'blue', '20% mix': 'red'}
    path_types = ['S1->S2', 'S2->S3', 'S1->S3']
    
    for i, path_type in enumerate(path_types):
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        has_data = False
        
        # ç¬¬1åˆ—ï¼šEdgeæƒé‡å¯¹æ¯”
        ax = axes[i, 0]
        for model_name, model_data in evolution_data.items():
            if model_name in model_data and path_type in model_data[model_name]:
                edge_weights = model_data[model_name][path_type]['edge']
                valid_iters = [it for it, w in zip(iterations, edge_weights) if not np.isnan(w)]
                valid_weights = [w for w in edge_weights if not np.isnan(w)]
                
                if valid_weights and path_type != 'S1->S3':
                    ax.plot(valid_iters, valid_weights, marker='o', label=model_name,
                           color=colors.get(model_name, 'gray'), linewidth=2, markersize=5, alpha=0.8)
                    has_data = True
        
        ax.set_ylabel(f'{path_type}', fontsize=12, fontweight='bold')
        if i == 0:
            ax.set_title('Average Edge Weight', fontsize=13)
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
        if has_data and i == 0:
            ax.legend(loc='best')
        
        # ç±»ä¼¼å¤„ç†ç¬¬2åˆ—å’Œç¬¬3åˆ—...
        # [çœç•¥é‡å¤ä»£ç ]
    
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    
    # ä¿å­˜å›¾ç‰‡
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_path = os.path.join(save_dir, f'weight_gap_comparison_0vs20_{timestamp}.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Plot saved to: {save_path}")
    plt.close()

# ==================== 4. ä¸»å‡½æ•°ï¼ˆä¿®å¤ç‰ˆï¼‰ ====================

def main():
    """ä¸»å‡½æ•° - ä¿®å¤ç‰ˆ"""
    print("\n" + "="*80)
    print("ğŸ”¬ WEIGHT GAP COMPARISON: 0% vs 20% MIX MODELS (FIXED)")
    print("="*80)
    
    # é…ç½®å‚æ•°
    d_model = 92
    iterations = list(range(5000, 51000, 5000))
    save_dir = 'weight_gap_analysis_0vs20'
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"\nğŸ“‹ Configuration:")
    print(f"  Model dimension: d={d_model}")
    print(f"  Vocab size: 92 (fixed)")
    print(f"  Device: CPU (to avoid CUDA errors)")
    print(f"  Iterations: {iterations[0]} to {iterations[-1]}")
    print(f"  Output directory: {save_dir}")
    
    # åˆå§‹åŒ–é…ç½®
    configs = {
        '0% mix': ModelConfig(mix_ratio=0, d_model=d_model),
        '20% mix': ModelConfig(mix_ratio=20, d_model=d_model)
    }
    
    # æ”¶é›†æ¼”åŒ–æ•°æ®
    print("\n" + "="*80)
    print("ğŸ“ˆ COLLECTING EVOLUTION DATA")
    print("="*80)
    
    evolution_data = collect_evolution_data(configs, iterations)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    has_any_data = False
    for model_name in evolution_data:
        for path_type in evolution_data[model_name]:
            if any(not np.isnan(g) for g in evolution_data[model_name][path_type]['gap']):
                has_any_data = True
                break
    
    if not has_any_data:
        print("\nâŒ No valid data collected. Please check:")
        print("  1. Checkpoint files exist in the expected locations")
        print("  2. Model vocab_size matches (should be 92)")
        print("  3. Try running with CUDA_LAUNCH_BLOCKING=1 for debugging")
        return
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\n" + "="*80)
    print("ğŸ“Š GENERATING VISUALIZATIONS")
    print("="*80)
    
    plot_comprehensive_comparison(evolution_data, iterations, save_dir)
    
    print("\n" + "="*80)
    print("âœ… ANALYSIS COMPLETE!")
    print("="*80)

if __name__ == "__main__":
    main()