#!/usr/bin/env python3
"""
analyze_alpha_weight_gaps.py
åˆ†æTrack Aæ‰€æœ‰alphaå€¼çš„weight gapæ¼”åŒ–
åŸºäº1å±‚1å¤´æ¨¡å‹çš„åŸå§‹weight gapè®¡ç®—æ–¹æ³•
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import pickle
import os
import glob
import networkx as nx
from tqdm import tqdm
import json
from datetime import datetime
import seaborn as sns

try:
    from model import GPTConfig, GPT
except ImportError:
    print("âŒ Error: Cannot import 'model.py'")
    exit()

# ==================== 1. é…ç½®ä¸è¾…åŠ©å‡½æ•° ====================

class AlphaModelConfig:
    """Alphaæ¨¡å‹é…ç½®ç±» - 1å±‚1å¤´ç‰ˆæœ¬"""
    def __init__(self, alpha_value, track='A', d_model=92):
        self.alpha = alpha_value
        self.track = track
        self.d_model = d_model
        self.device = torch.device('cpu')
        
        # æ¨¡å‹å‚æ•° - æ³¨æ„æ˜¯1å±‚1å¤´ï¼
        self.n_layer = 1
        self.n_head = 1
        self.n_embd = d_model
        self.vocab_size = 92
        
        # æ•°æ®ç›®å½•
        self.data_dir = f'data/simple_graph/alpha_track_{track}_{alpha_value:.1f}'
        
        # åŠ è½½èŠ‚ç‚¹åˆ†ç»„å’Œå›¾ç»“æ„
        self.load_stage_info()
        self.load_graph_structure()
    
    def load_stage_info(self):
        """åŠ è½½èŠ‚ç‚¹åˆ†ç»„ä¿¡æ¯"""
        stage_info_path = os.path.join(self.data_dir, 'stage_info.pkl')
        
        if not os.path.exists(stage_info_path):
            # å¦‚æœå½“å‰ç›®å½•æ²¡æœ‰ï¼Œä½¿ç”¨åŸå§‹ç›®å½•çš„
            stage_info_path = 'data/simple_graph/standardized_alpine_90_seed42/stage_info.pkl'
        
        with open(stage_info_path, 'rb') as f:
            stage_info = pickle.load(f)
        
        self.S1, self.S2, self.S3 = stage_info['stages']
        
        # è½¬æ¢ä¸ºé›†åˆ
        self.S1_set = set(self.S1)
        self.S2_set = set(self.S2)
        self.S3_set = set(self.S3)
        
        # åˆ›å»ºèŠ‚ç‚¹åˆ°tokençš„æ˜ å°„ï¼ˆ+2æ˜¯å› ä¸ºtoken 0,1æ˜¯ç‰¹æ®Štokenï¼‰
        self.node_to_token = {node: node + 2 for node in range(90)}
        self.token_to_node = {token: node for node, token in self.node_to_token.items()}
        
        # S1, S2, S3çš„tokenç´¢å¼•
        self.S1_tokens = [self.node_to_token[n] for n in self.S1]
        self.S2_tokens = [self.node_to_token[n] for n in self.S2]
        self.S3_tokens = [self.node_to_token[n] for n in self.S3]
    
    def load_graph_structure(self):
        """åŠ è½½å›¾ç»“æ„"""
        graph_path = os.path.join(self.data_dir, 'composition_graph.graphml')
        
        if not os.path.exists(graph_path):
            graph_path = 'data/simple_graph/standardized_alpine_90_seed42/composition_graph.graphml'
        
        G = nx.read_graphml(graph_path)
        
        # ç¡®ä¿èŠ‚ç‚¹æ˜¯æ•´æ•°
        if isinstance(list(G.nodes())[0], str):
            self.G = nx.relabel_nodes(G, {node: int(node) for node in G.nodes()})
        else:
            self.G = G
        
        # åˆ›å»ºé‚»æ¥çŸ©é˜µ
        self.A_true = np.zeros((self.vocab_size, self.vocab_size))
        
        for edge in self.G.edges():
            source_token = self.node_to_token[edge[0]]
            target_token = self.node_to_token[edge[1]]
            self.A_true[source_token, target_token] = 1

def find_alpha_checkpoint(alpha_value, iteration, base_dir='out'):
    """æŸ¥æ‰¾ç‰¹å®šalphaå€¼çš„checkpoint"""
    alpha_int = int(alpha_value * 100)
    pattern = f"{base_dir}/trackA_alpha{alpha_int}_d92_seed42_*/ckpt_{iteration}.pt"
    
    files = glob.glob(pattern)
    if files:
        return files[0]
    return None

def extract_W_M_prime(checkpoint_path, config):
    """æå–W'_MçŸ©é˜µ - ä½¿ç”¨æ‚¨åŸå§‹çš„1å±‚1å¤´ç‰ˆæœ¬"""
    try:
        checkpoint = torch.load(checkpoint_path, map_location=config.device, weights_only=False)
        
        model_args = checkpoint.get('model_args', {})
        if not model_args:
            model_args = {
                'n_layer': config.n_layer,
                'n_head': config.n_head,
                'n_embd': config.n_embd,
                'vocab_size': config.vocab_size,
                'block_size': 512,
                'dropout': 0.0,
                'bias': False
            }
        
        model_args['vocab_size'] = config.vocab_size
        
        gptconf = GPTConfig(**model_args)
        model = GPT(gptconf).to(config.device)
        model.load_state_dict(checkpoint['model'], strict=False)
        model.eval()
        
        W_M_prime = []
        with torch.no_grad():
            for i in range(config.vocab_size):
                token_emb = model.transformer.wte(torch.tensor([i], device=config.device))
                ffn_out = model.transformer.h[0].mlp(token_emb)
                combined = token_emb + ffn_out
                logits = model.lm_head(combined)
                W_M_prime.append(logits.squeeze().cpu().numpy()[:config.vocab_size])
        
        return np.array(W_M_prime)
        
    except Exception as e:
        print(f"    Error extracting W_M_prime: {e}")
        raise

def calculate_path_statistics(W_M_prime, config, path_type):
    """è®¡ç®—è·¯å¾„ç»Ÿè®¡ - ä½¿ç”¨æ­£ç¡®çš„èŠ‚ç‚¹åˆ†ç»„"""
    if path_type == 'S1->S2':
        source_tokens = config.S1_tokens
        target_tokens = config.S2_tokens
    elif path_type == 'S2->S3':
        source_tokens = config.S2_tokens
        target_tokens = config.S3_tokens
    elif path_type == 'S1->S3':
        source_tokens = config.S1_tokens
        target_tokens = config.S3_tokens
    else:
        raise ValueError(f"Invalid path_type: {path_type}")
    
    # æå–ç›¸å…³çš„å­çŸ©é˜µ
    W_sub = W_M_prime[np.ix_(source_tokens, target_tokens)]
    A_sub = config.A_true[np.ix_(source_tokens, target_tokens)]
    
    # åˆ›å»ºæ©ç 
    edge_mask = (A_sub == 1)
    non_edge_mask = (A_sub == 0)
    
    stats = {
        'num_edges': np.sum(edge_mask),
        'num_non_edges': np.sum(non_edge_mask)
    }
    
    if stats['num_edges'] > 0:
        stats['avg_edge_weight'] = np.mean(W_sub[edge_mask])
        stats['std_edge_weight'] = np.std(W_sub[edge_mask])
    else:
        stats['avg_edge_weight'] = np.nan
        stats['std_edge_weight'] = np.nan
    
    if stats['num_non_edges'] > 0:
        stats['avg_non_edge_weight'] = np.mean(W_sub[non_edge_mask])
        stats['std_non_edge_weight'] = np.std(W_sub[non_edge_mask])
    else:
        stats['avg_non_edge_weight'] = 0
        stats['std_non_edge_weight'] = 0
    
    if stats['num_edges'] > 0:
        stats['gap'] = stats['avg_edge_weight'] - stats['avg_non_edge_weight']
    else:
        stats['gap'] = np.nan
    
    return stats

# ==================== 2. æ•°æ®æ”¶é›†å‡½æ•° ====================

def collect_alpha_evolution_data(alpha_values, iterations):
    """æ”¶é›†æ‰€æœ‰alphaå€¼çš„æ¼”åŒ–æ•°æ®"""
    all_data = {}
    
    for alpha in tqdm(alpha_values, desc="Processing alpha values"):
        print(f"\nğŸ“Š Processing Î±={alpha:.1f} model...")
        
        config = AlphaModelConfig(alpha_value=alpha, track='A')
        
        model_data = {
            'S1->S2': {'edge': [], 'non_edge': [], 'gap': []},
            'S2->S3': {'edge': [], 'non_edge': [], 'gap': []},
            'S1->S3': {'edge': [], 'non_edge': [], 'gap': []}
        }
        
        found_checkpoints = 0
        for iteration in iterations:
            checkpoint_path = find_alpha_checkpoint(alpha, iteration)
            
            if checkpoint_path is None:
                for path_type in model_data.keys():
                    model_data[path_type]['edge'].append(np.nan)
                    model_data[path_type]['non_edge'].append(np.nan)
                    model_data[path_type]['gap'].append(np.nan)
                continue
            
            try:
                W_M_prime = extract_W_M_prime(checkpoint_path, config)
                found_checkpoints += 1
                
                for path_type in ['S1->S2', 'S2->S3', 'S1->S3']:
                    stats = calculate_path_statistics(W_M_prime, config, path_type)
                    model_data[path_type]['edge'].append(stats['avg_edge_weight'])
                    model_data[path_type]['non_edge'].append(stats['avg_non_edge_weight'])
                    model_data[path_type]['gap'].append(stats['gap'])
                    
            except Exception as e:
                print(f"  âš ï¸ Error at iteration {iteration}: {e}")
                for path_type in model_data.keys():
                    model_data[path_type]['edge'].append(np.nan)
                    model_data[path_type]['non_edge'].append(np.nan)
                    model_data[path_type]['gap'].append(np.nan)
        
        print(f"  âœ“ Found {found_checkpoints}/{len(iterations)} checkpoints")
        all_data[alpha] = model_data
    
    return all_data

# ==================== 3. å¯è§†åŒ–å‡½æ•° ====================

def plot_alpha_weight_gaps(all_data, alpha_values, iterations, save_dir):
    """ç”Ÿæˆalpha weight gapåˆ†æå›¾"""
    
    # 1. Weight Gapæ¼”åŒ–å›¾
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('Weight Gap Evolution Across Alpha Values (Track A, 1L-1H Model)', fontsize=16, fontweight='bold')
    
    path_types = ['S1->S2', 'S2->S3', 'S1->S3']
    colors = plt.cm.viridis(np.linspace(0, 1, len(alpha_values)))
    
    for i, path_type in enumerate(path_types):
        ax = axes[i]
        
        for j, alpha in enumerate(alpha_values):
            gaps = all_data[alpha][path_type]['gap']
            valid_iters = [it for it, g in zip(iterations, gaps) if not np.isnan(g)]
            valid_gaps = [g for g in gaps if not np.isnan(g)]
            
            if valid_gaps:
                ax.plot(valid_iters, valid_gaps, marker='o', label=f'Î±={alpha:.1f}',
                       color=colors[j], linewidth=1.5, markersize=3, alpha=0.7)
        
        ax.set_title(f'{path_type} Weight Gap', fontsize=14, fontweight='bold')
        ax.set_xlabel('Training Iterations', fontsize=12)
        ax.set_ylabel('Weight Gap', fontsize=12)
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1)
        
        if i == 2:
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'alpha_weight_gap_evolution.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Evolution plot saved to: {save_path}")
    plt.show()
    
    # 2. æœ€ç»ˆWeight Gap vs Alphaå›¾
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    fig.suptitle('Final Weight Gap vs Alpha (1L-1H Model)', fontsize=16, fontweight='bold')
    
    final_gaps = {path_type: [] for path_type in path_types}
    
    for alpha in alpha_values:
        for path_type in path_types:
            gaps = all_data[alpha][path_type]['gap']
            valid_gaps = [g for g in gaps if not np.isnan(g)]
            if valid_gaps:
                final_gaps[path_type].append(valid_gaps[-1])
            else:
                final_gaps[path_type].append(np.nan)
    
    # ç»˜åˆ¶æ›²çº¿
    for path_type in path_types:
        valid_alphas = [a for a, g in zip(alpha_values, final_gaps[path_type]) if not np.isnan(g)]
        valid_gaps = [g for g in final_gaps[path_type] if not np.isnan(g)]
        
        if valid_gaps:
            ax.plot(valid_alphas, valid_gaps, marker='o', label=path_type, 
                   linewidth=2, markersize=8)
    
    ax.set_xlabel('Alpha (Interpolation Parameter)', fontsize=14)
    ax.set_ylabel('Final Weight Gap', fontsize=14)
    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    ax.grid(True, alpha=0.3)
    ax.legend(fontsize=12)
    
    for alpha in [0.0, 0.5, 1.0]:
        ax.axvline(x=alpha, color='gray', linestyle=':', alpha=0.3)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'final_weight_gap_vs_alpha.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Final gap plot saved to: {save_path}")
    plt.show()
    
    # 3. çƒ­å›¾
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('Weight Gap Heatmaps (1L-1H Model)', fontsize=16, fontweight='bold')
    
    for i, path_type in enumerate(path_types):
        ax = axes[i]
        
        gap_matrix = []
        for alpha in alpha_values:
            gaps = all_data[alpha][path_type]['gap']
            gap_matrix.append(gaps)
        
        gap_matrix = np.array(gap_matrix)
        
        im = ax.imshow(gap_matrix, aspect='auto', cmap='RdBu_r', 
                      vmin=-0.5, vmax=0.5, interpolation='nearest')
        
        ax.set_title(f'{path_type}', fontsize=14)
        ax.set_xlabel('Training Iterations', fontsize=12)
        ax.set_ylabel('Alpha', fontsize=12)
        
        ax.set_xticks(range(0, len(iterations), 2))
        ax.set_xticklabels([f'{it//1000}k' for it in iterations[::2]])
        ax.set_yticks(range(len(alpha_values)))
        ax.set_yticklabels([f'{a:.1f}' for a in alpha_values])
        
        plt.colorbar(im, ax=ax, label='Weight Gap')
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'weight_gap_heatmaps.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Heatmap saved to: {save_path}")
    plt.show()

def print_summary_statistics(all_data, alpha_values):
    """æ‰“å°æ±‡æ€»ç»Ÿè®¡"""
    print("\n" + "="*80)
    print("ğŸ“Š SUMMARY STATISTICS (1L-1H Model)")
    print("="*80)
    
    path_types = ['S1->S2', 'S2->S3', 'S1->S3']
    
    final_gaps = {path_type: {} for path_type in path_types}
    
    for alpha in alpha_values:
        for path_type in path_types:
            gaps = all_data[alpha][path_type]['gap']
            valid_gaps = [g for g in gaps if not np.isnan(g)]
            if valid_gaps:
                final_gaps[path_type][alpha] = valid_gaps[-1]
    
    print("\nFinal Weight Gaps:")
    print("-" * 60)
    print(f"{'Alpha':<10} {'S1->S2':<15} {'S2->S3':<15} {'S1->S3':<15}")
    print("-" * 60)
    
    for alpha in alpha_values:
        row = f"{alpha:<10.1f}"
        for path_type in path_types:
            if alpha in final_gaps[path_type]:
                value = final_gaps[path_type][alpha]
                row += f"{value:<15.4f}"
                if path_type == 'S2->S3' and value < 0:
                    row = row[:-5] + "âš ï¸" + row[-4:]
            else:
                row += f"{'N/A':<15}"
        print(row)
    
    print("-" * 60)
    
    print("\nğŸ” Key Observations:")
    
    s23_gaps = final_gaps['S2->S3']
    negative_alphas = [a for a, g in s23_gaps.items() if g < 0]
    if negative_alphas:
        print(f"  â€¢ S2->S3 gap becomes negative at Î± â‰¥ {min(negative_alphas):.1f}")
    else:
        print(f"  â€¢ S2->S3 gap remains positive for all Î± values âœ…")
    
    s13_gaps = final_gaps['S1->S3']
    if s13_gaps:
        max_alpha = max(s13_gaps.items(), key=lambda x: x[1])[0]
        print(f"  â€¢ S1->S3 gap is maximum at Î± = {max_alpha:.1f}")

# ==================== 4. ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ”¬ ALPHA MIXING WEIGHT GAP ANALYSIS (1-Layer 1-Head Model)")
    print("="*80)
    
    # é…ç½®
    alpha_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    iterations = list(range(5000, 51000, 5000))
    save_dir = 'alpha_weight_gap_analysis_1L1H'
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"\nğŸ“‹ Configuration:")
    print(f"  â€¢ Model: 1 Layer, 1 Head, 92D")
    print(f"  â€¢ Alpha values: {alpha_values}")
    print(f"  â€¢ Iterations: {iterations}")
    print(f"  â€¢ Output directory: {save_dir}")
    
    # æ”¶é›†æ•°æ®
    print("\n" + "="*60)
    print("Collecting data from checkpoints...")
    print("="*60)
    
    all_data = collect_alpha_evolution_data(alpha_values, iterations)
    
    # ä¿å­˜åŸå§‹æ•°æ®
    with open(os.path.join(save_dir, 'alpha_weight_gap_data.pkl'), 'wb') as f:
        pickle.dump(all_data, f)
    print(f"âœ… Raw data saved to: {save_dir}/alpha_weight_gap_data.pkl")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\n" + "="*60)
    print("Generating visualizations...")
    print("="*60)
    
    plot_alpha_weight_gaps(all_data, alpha_values, iterations, save_dir)
    
    # æ‰“å°ç»Ÿè®¡
    print_summary_statistics(all_data, alpha_values)
    
    print("\n" + "="*80)
    print("âœ… ANALYSIS COMPLETE!")
    print(f"ğŸ“ All results saved to: {save_dir}/")
    print("="*80)

if __name__ == "__main__":
    main()